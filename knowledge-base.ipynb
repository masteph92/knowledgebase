{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Base\n",
    "\n",
    "## Auswahl der Sprachmodelle \n",
    "\n",
    "### Llama 3.3 (8B Version)\n",
    "- **Leistung**: Starke Performance für RAG-Anwendungen mit guter Kontextverarbeitung\n",
    "- **Größe**: Mit 8B Parametern effizient auf Consumer-Hardware lauffähig\n",
    "- **Lizenz**: Permissive Lizenz ermöglicht kommerzielle Nutzung\n",
    "- **Aktualität**: Neues Modell mit modernem Trainingsdatenset und verbesserter Instruction-Following-Fähigkeit\n",
    "\n",
    "### Mistral 7B Instruct\n",
    "- **Effizienz**: Ausgezeichnetes Leistungs-Größen-Verhältnis\n",
    "- **Spezialisierung**: Optimiert für Instruction-Following und Kontextverständnis\n",
    "- **Architektur**: Gruppenweise Rotation der Aufmerksamkeit für verbesserte Verarbeitung langer Dokumente\n",
    "- **Community-Support**: Breite Nutzerbasis und dokumentierte Anwendungsfälle für RAG\n",
    "\n",
    "### Phi-4 (Mini)\n",
    "- **Ressourcenschonung**: Kleines Modell (3.8B) für Systeme mit begrenzten Ressourcen\n",
    "- **Effizienz**: Hervorragende Leistung trotz geringer Größe\n",
    "- **Antwortqualität**: Gute Formulierungsfähigkeit bei unternehmensbezogenen Inhalten\n",
    "- **Kompatibilität**: Geringer VRAM-Bedarf macht es auf verschiedenen Systemen einsetzbar\n",
    "\n",
    "Dieser Mix bietet eine gute Balance zwischen Performance, Ressourcenbedarf und verschiedenen Architekturen für einen aussagekräftigen Vergleich.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Einrichtung der Knowledge Base\n",
    "\n",
    "### Verzeichnisstruktur + Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import fitz\n",
    "import re\n",
    "from typing import List\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "BASE_DIR = Path(\"knowledge-base\")\n",
    "IMPORT_DIR = BASE_DIR / \"import\"\n",
    "PROCESSED_DIR = BASE_DIR / \"processed\"\n",
    "INPUT_DIR = BASE_DIR / \"embeddings-ready\"\n",
    "VECTOR_DB_DIR = BASE_DIR / \"vector-stores\"\n",
    "VECTOR_DB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for dir_path in [BASE_DIR, IMPORT_DIR, PROCESSED_DIR, INPUT_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dokumenten-Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF-Dateien: 0\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "def list_pdf_files(directory):\n",
    "    return [f for f in directory.glob(\"*.pdf\")]\n",
    "  \n",
    "pdf_files = list_pdf_files(IMPORT_DIR)\n",
    "\n",
    "print(f\"PDF-Dateien: {len(pdf_files)}\")\n",
    "for pdf in pdf_files[:5]:  # Zeige die ersten 5 Dateien\n",
    "    print(f\" - {pdf.name}\")\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text_content = []\n",
    "    \n",
    "    for page_num in range(len(document)):\n",
    "        page = document[page_num]\n",
    "        text = page.get_text()\n",
    "        \n",
    "        # Grundlegende Textbereinigung\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Mehrfach-Leerzeichen entfernen\n",
    "        text = text.strip()\n",
    "        \n",
    "        if text:\n",
    "            text_content.append(f\"--- Seite {page_num + 1} ---\\n{text}\")\n",
    "    \n",
    "    document.close()\n",
    "    \n",
    "    # Gesamten Text zusammenführen\n",
    "    processed_text = \"\\n\\n\".join(text_content)\n",
    "    \n",
    "    # Weitere Bereinigungen für bessere LLM-Verarbeitung\n",
    "    processed_text = re.sub(r'([.!?])\\s*(\\w)', r'\\1\\n\\2', processed_text)  # Satzenden mit Zeilenumbrüchen\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# 4. Dateien verarbeiten und verschieben\n",
    "processed_files = []\n",
    "\n",
    "for pdf_path in pdf_files:\n",
    "    print(f\"Verarbeite: {pdf_path.name}\")\n",
    "    \n",
    "    # Text extrahieren und aufbereiten\n",
    "    processed_text = process_pdf(pdf_path)\n",
    "    \n",
    "    # Ausgabedatei im embeddings-ready Verzeichnis erstellen\n",
    "    output_file = INPUT_DIR / f\"{pdf_path.stem}.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(processed_text)\n",
    "    \n",
    "    # Originaldatei in processed-Verzeichnis verschieben\n",
    "    target_path = PROCESSED_DIR / pdf_path.name\n",
    "    shutil.move(pdf_path, target_path)\n",
    "    \n",
    "    processed_files.append({\n",
    "        \"original_file\": pdf_path.name,\n",
    "        \"processed_file\": output_file.name,\n",
    "        \"size_kb\": round(output_file.stat().st_size / 1024, 2)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung der Content Embeddings \n",
    "\n",
    "### Leitfragen zur Bewertung\n",
    "\n",
    "### Ausgewählte Embeddings\n",
    "\n",
    "### Ergebnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geladen: 10 Dokumente\n",
      "Dokumente in 222 Chunks aufgeteilt.\n",
      "\n",
      "--- Erstelle Vektordatenbank: Word-Level mit all-MiniLM-L6-v2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\campus\\mle\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vektordatenbank wurde in knowledge-base\\vector-stores\\Word-Level_all-MiniLM-L6-v2 gespeichert.\n",
      "\n",
      "--- Erstelle Vektordatenbank: Sentence-Level mit all-mpnet-base-v2 ---\n",
      "Vektordatenbank wurde in knowledge-base\\vector-stores\\Sentence-Level_all-mpnet-base-v2 gespeichert.\n",
      "\n",
      "--- Erstelle Vektordatenbank: Document-Level mit multilingual-e5-large ---\n",
      "Vektordatenbank wurde in knowledge-base\\vector-stores\\Document-Level_multilingual-e5-large gespeichert.\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_MODELS = {\n",
    "    \"Word-Level\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"Sentence-Level\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"Document-Level\": \"intfloat/multilingual-e5-large\"\n",
    "}\n",
    "\n",
    "# Fixed values\n",
    "CONTEXT_WINDOW = 8192\n",
    "TOKEN_LIMIT = 4096\n",
    "\n",
    "# Document loading function (unchanged)\n",
    "def load_documents(directory: Path) -> List[Document]:\n",
    "    \"\"\"Lädt alle Textdateien aus dem angegebenen Verzeichnis.\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for file_path in directory.glob(\"*.txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=text,\n",
    "            metadata={\"source\": file_path.name}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    print(f\"Geladen: {len(documents)} Dokumente\")\n",
    "    return documents\n",
    "\n",
    "# Text-Splitter functions (only keeping word-level for now)\n",
    "def prepare_word_level_chunks(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"Teilt Dokumente in überlappende Wortgruppen auf.\"\"\"\n",
    "    # Use fixed values divided by 2 as in original logic\n",
    "    max_chunk_size = min(CONTEXT_WINDOW // 2, TOKEN_LIMIT)\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_chunk_size,\n",
    "        chunk_overlap=50,  # Fixed overlap\n",
    "        length_function=len\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "  \n",
    "def create_vector_database(documents: List[Document], embedding_type: str, \n",
    "                          model_name: str, model_path: str, vector_db_dir: Path):\n",
    "    \"\"\"Erstellt eine Vektordatenbank mit den angegebenen Embeddings.\"\"\"\n",
    "    print(f\"\\n--- Erstelle Vektordatenbank: {embedding_type} mit {model_name} ---\")\n",
    "    \n",
    "    # Embeddings initialisieren\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_path)\n",
    "    \n",
    "    # Vektordatenbank-Pfad definieren\n",
    "    db_path = vector_db_dir / f\"{embedding_type}_{model_name.replace('/', '_')}\"\n",
    "    \n",
    "    # Vektordatenbank erstellen\n",
    "    db = Chroma.from_documents(\n",
    "        documents, \n",
    "        embeddings, \n",
    "        persist_directory=str(db_path)\n",
    "    )\n",
    "    \n",
    "    print(f\"Vektordatenbank wurde in {db_path} gespeichert.\")\n",
    "    return db\n",
    "  \n",
    "documents = load_documents(INPUT_DIR)\n",
    "if not documents:\n",
    "    print(\"Keine Dokumente gefunden!\")\n",
    "else:\n",
    "  # Dokumente in Chunks aufteilen (einmalig)\n",
    "  word_chunks = prepare_word_level_chunks(documents)\n",
    "  print(f\"Dokumente in {len(word_chunks)} Chunks aufgeteilt.\")\n",
    "  \n",
    "  # Für jedes Embedding-Modell eine Vektordatenbank erstellen\n",
    "  for embedding_type, model_path in EMBEDDING_MODELS.items():\n",
    "      model_name = model_path.split(\"/\")[-1]\n",
    "      \n",
    "      # Vektordatenbank erstellen\n",
    "      create_vector_database(\n",
    "          word_chunks, embedding_type, model_name, model_path, VECTOR_DB_DIR\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung des Chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking-Methoden mit Document-Level Embedding-Modell\n",
      "Geladen: 10 Dokumente\n",
      "\n",
      "=== Chunking-Methode: fixed_size ===\n",
      "\n",
      "--- Erstelle Vektordatenbank mit Chunking-Methode: fixed_size ---\n",
      "Chunks erstellt: 759\n",
      "\n",
      "Beispiel-Chunks:\n",
      "Chunk 1, Größe: 747 Zeichen\n",
      "Quelle: 1-Challenges_and_Reliability_of_Predictive_Maintenance.txt\n",
      "Inhalt: --- Seite 1 ---\n",
      "See discussions, stats, and author profiles for this publication at: https://www.\n",
      "researchgate.\n",
      "net/publication/331951459 Challenges a...\n",
      "\n",
      "Chunk 2, Größe: 952 Zeichen\n",
      "Quelle: 1-Challenges_and_Reliability_of_Predictive_Maintenance.txt\n",
      "Inhalt: --- Seite 3 ---\n",
      "Declaration of Authorship I, Vincent F.\n",
      "A.\n",
      "Meyer zu Wickern, declare that this thesis titled, ‘Challenges and Reliability of Predictiv...\n",
      "\n",
      "Vektordatenbank wurde in knowledge-base\\vector-stores\\chunking_fixed_size gespeichert.\n",
      "\n",
      "=== Chunking-Methode: sentence ===\n",
      "\n",
      "--- Erstelle Vektordatenbank mit Chunking-Methode: sentence ---\n",
      "Chunks erstellt: 745\n",
      "\n",
      "Beispiel-Chunks:\n",
      "Chunk 1, Größe: 747 Zeichen\n",
      "Quelle: 1-Challenges_and_Reliability_of_Predictive_Maintenance.txt\n",
      "Inhalt: --- Seite 1 ---\n",
      "See discussions, stats, and author profiles for this publication at: https://www.\n",
      "researchgate.\n",
      "net/publication/331951459 Challenges a...\n",
      "\n",
      "Chunk 2, Größe: 952 Zeichen\n",
      "Quelle: 1-Challenges_and_Reliability_of_Predictive_Maintenance.txt\n",
      "Inhalt: --- Seite 3 ---\n",
      "Declaration of Authorship I, Vincent F.\n",
      "A.\n",
      "Meyer zu Wickern, declare that this thesis titled, ‘Challenges and Reliability of Predictiv...\n",
      "\n",
      "Vektordatenbank wurde in knowledge-base\\vector-stores\\chunking_sentence gespeichert.\n",
      "\n",
      "=== Chunking-Methode: paragraph ===\n",
      "\n",
      "--- Erstelle Vektordatenbank mit Chunking-Methode: paragraph ---\n",
      "Chunks erstellt: 407\n",
      "\n",
      "Beispiel-Chunks:\n",
      "Chunk 1, Größe: 1701 Zeichen\n",
      "Quelle: 1-Challenges_and_Reliability_of_Predictive_Maintenance.txt\n",
      "Inhalt: --- Seite 1 ---\n",
      "See discussions, stats, and author profiles for this publication at: https://www.\n",
      "researchgate.\n",
      "net/publication/331951459 Challenges a...\n",
      "\n",
      "Chunk 2, Größe: 1850 Zeichen\n",
      "Quelle: 1-Challenges_and_Reliability_of_Predictive_Maintenance.txt\n",
      "Inhalt: --- Seite 4 ---\n",
      "RHEIN-WAAL UNIVERSITY OF APPLIED SCIENCES Abstract Faculty of Communication and Environment by Vincent F.\n",
      "A.\n",
      "Meyer zu Wickern Predicti...\n",
      "\n",
      "Vektordatenbank wurde in knowledge-base\\vector-stores\\chunking_paragraph gespeichert.\n"
     ]
    }
   ],
   "source": [
    "SELECTED_EMBEDDING = \"Document-Level\"\n",
    "EMBEDDING_MODEL = EMBEDDING_MODELS[SELECTED_EMBEDDING]\n",
    "\n",
    "CHUNKING_METHODS = {\n",
    "    \"fixed_size\": RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=100\n",
    "    ),\n",
    "    \n",
    "    \"sentence\": RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \";\", \":\"],\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=0\n",
    "    ),\n",
    "    \n",
    "    \"paragraph\": RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\"],\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "}\n",
    "\n",
    "def create_vector_database_with_chunking(documents: List[Document], method_name: str, \n",
    "                                        splitter, vector_db_dir: Path):\n",
    "    \"\"\"Erstellt eine Vektordatenbank mit der angegebenen Chunking-Methode.\"\"\"\n",
    "    print(f\"\\n--- Erstelle Vektordatenbank mit Chunking-Methode: {method_name} ---\")\n",
    "    \n",
    "    # Dokumente chunken\n",
    "    chunks = []\n",
    "    try:\n",
    "        for doc in documents:\n",
    "            if hasattr(splitter, 'split_documents'):\n",
    "                # Für TextSplitter, die direkt Dokumente unterstützen\n",
    "                doc_chunks = splitter.split_documents([doc])\n",
    "            else:\n",
    "                # Für TextSplitter, die nur Text unterstützen\n",
    "                text_chunks = splitter.split_text(doc.page_content)\n",
    "                doc_chunks = [\n",
    "                    Document(page_content=chunk, metadata=doc.metadata)\n",
    "                    for chunk in text_chunks\n",
    "                ]\n",
    "            chunks.extend(doc_chunks)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Chunking mit {method_name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Chunks erstellt: {len(chunks)}\")\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"Keine Chunks erzeugt!\")\n",
    "        return None\n",
    "    \n",
    "    # Beispiel-Chunks anzeigen\n",
    "    print(\"\\nBeispiel-Chunks:\")\n",
    "    for i in range(min(2, len(chunks))):\n",
    "        print(f\"Chunk {i+1}, Größe: {len(chunks[i].page_content)} Zeichen\")\n",
    "        print(f\"Quelle: {chunks[i].metadata.get('source', 'Unbekannt')}\")\n",
    "        print(f\"Inhalt: {chunks[i].page_content[:150]}...\\n\")\n",
    "    \n",
    "    # Embeddings erstellen und in Vector Store speichern\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "    \n",
    "    db_path = vector_db_dir / f\"chunking_{method_name}\"\n",
    "    \n",
    "    db = Chroma.from_documents(\n",
    "        chunks, \n",
    "        embeddings, \n",
    "        persist_directory=str(db_path)\n",
    "    )\n",
    "    \n",
    "    print(f\"Vektordatenbank wurde in {db_path} gespeichert.\")\n",
    "    return db\n",
    "  \n",
    "print(f\"Chunking-Methoden mit {SELECTED_EMBEDDING} Embedding-Modell\")\n",
    "  \n",
    "# Dokumente laden\n",
    "documents = load_documents(INPUT_DIR)\n",
    "if not documents:\n",
    "    print(\"Keine Dokumente gefunden!\")\n",
    "else:\n",
    "  # Chunking-Methoden testen\n",
    "  for method_name, splitter in CHUNKING_METHODS.items():\n",
    "      print(f\"\\n=== Chunking-Methode: {method_name} ===\")\n",
    "      create_vector_database_with_chunking(documents, method_name, splitter, VECTOR_DB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An F1-Score in anomaly detection (also known as precision-recall curve AUC) combines both Precision and Recall into one metric. It helps assess how well-balanced two measures are for a particular classification system, particularly useful when the class distribution may be imbalanced.\n",
      "\n",
      "Precision is defined as true positive rate or sensitivity: TPR = TP/(TP+FN). In this context it refers to correctly identified anomalies (True Positives) divided by all predicted positives. The F1-Score combines Precision and Recall with equal weighting into one metric.\n",
      "Recall, on the other hand, measures how many actual anomaly events are detected as such; in other words TPR = TP/(TP+FN). In this context it refers to correctly identified anomalies (True Positives) divided by all positive samples. The F1-Score combines Precision and Recall with equal weighting into one metric.\n",
      "An adjusted version of the F1 score accounts for both false positives, denoted as FP's, and missed detections FN' 's; thus allowing us to measure performance in a more comprehensive way while also taking advantage of detecting overlapping anomalies or partial events. Overall this allows anomaly detection systems better optimization through improved metrics that accurately reflect real-world operational conditions.\n",
      "An F1-Score for anomaly detection is especially crucial when dealing with imbalanced datasets, as precision-recall curves (PR-C) can provide valuable insights into the performance at different cutoff points on a continuous score metric generated by an algorithm. This helps in identifying optimal thresholds to maximize accuracy while minimizing false alarms or missed detections.\n"
     ]
    }
   ],
   "source": [
    "def ask_ollama(query, vector_db_dir=VECTOR_DB_DIR):\n",
    "    \"\"\"RAG mit Phi-4-mini über Ollama\"\"\"\n",
    "    import requests\n",
    "    \n",
    "    # DB laden und abfragen\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    "    db = Chroma(persist_directory=str(vector_db_dir / \"chunking_sentence\"), embedding_function=embeddings)\n",
    "    results = db.similarity_search(query, k=3)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "    \n",
    "    # Prompt erstellen\n",
    "    prompt = f\"\"\"Basierend auf folgendem Kontext, beantworte die Frage.\n",
    "\n",
    "Kontext:\n",
    "{context}\n",
    "\n",
    "Frage: {query}\n",
    "\n",
    "Antwort:\"\"\"\n",
    "    \n",
    "    # Ollama API anfragen\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\"model\": \"phi4-mini\", \"prompt\": prompt, \"stream\": False}\n",
    "        )\n",
    "        return response.json().get(\"response\", \"\") if response.status_code == 200 else f\"Fehler: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Fehler: {str(e)}\"\n",
    "\n",
    "# Beispiel\n",
    "query = \"What is an F1-Score in anomaly detection?\"\n",
    "print(ask_ollama(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
