{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Base\n",
    "\n",
    "## Auswahl der Sprachmodelle \n",
    "\n",
    "### llama3:8b\n",
    "- **Leistung**: Starke Performance für RAG-Anwendungen mit guter Kontextverarbeitung\n",
    "- **Größe**: Mit 8B Parametern effizient auf Consumer-Hardware lauffähig\n",
    "- **Lizenz**: Permissive Lizenz ermöglicht kommerzielle Nutzung\n",
    "- **Aktualität**: Neues Modell mit modernem Trainingsdatenset und verbesserter Instruction-Following-Fähigkeit\n",
    "\n",
    "### mistral:7b-instruct\n",
    "- **Effizienz**: Ausgezeichnetes Leistungs-Größen-Verhältnis\n",
    "- **Spezialisierung**: Optimiert für Instruction-Following und Kontextverständnis\n",
    "- **Architektur**: Gruppenweise Rotation der Aufmerksamkeit für verbesserte Verarbeitung langer Dokumente\n",
    "- **Community-Support**: Breite Nutzerbasis und dokumentierte Anwendungsfälle für RAG\n",
    "\n",
    "### phi4-mini\n",
    "- **Ressourcenschonung**: Kleines Modell (3.8B) für Systeme mit begrenzten Ressourcen\n",
    "- **Effizienz**: Hervorragende Leistung trotz geringer Größe\n",
    "- **Antwortqualität**: Gute Formulierungsfähigkeit bei unternehmensbezogenen Inhalten\n",
    "- **Kompatibilität**: Geringer VRAM-Bedarf macht es auf verschiedenen Systemen einsetzbar\n",
    "\n",
    "Dieser Mix bietet eine gute Balance zwischen Performance, Ressourcenbedarf und verschiedenen Architekturen für einen aussagekräftigen Vergleich.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Einrichtung der Knowledge Base\n",
    "\n",
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import fitz\n",
    "import re\n",
    "from typing import List\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from langchain.schema import Document\n",
    "import time\n",
    "import psutil\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Folder structure\n",
    "BASE_DIR = Path(\"knowledge-base\")\n",
    "IMPORT_DIR = BASE_DIR / \"import\"\n",
    "PROCESSED_DIR = BASE_DIR / \"processed\"\n",
    "INPUT_DIR = BASE_DIR / \"embeddings-ready\"\n",
    "VECTOR_DB_DIR = BASE_DIR / \"vector-stores\"\n",
    "\n",
    "\n",
    "# Query Options\n",
    "CONTEXT_WINDOW = 8192\n",
    "TOKEN_LIMIT = 4096\n",
    "OLLAMA_MODEL = \"phi4-mini\"\n",
    "TEST_QUERY = \"Which callback function is called during training?\"\n",
    "EXPECTED_ANSWER = \"ModelCheckpoint\"\n",
    "\n",
    "# Define Embedding Configs\n",
    "EMBEDDING_CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"word_level\",\n",
    "        \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"db_path\": VECTOR_DB_DIR / \"word_level_db\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"sentence_level\",\n",
    "        \"model\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        \"db_path\": VECTOR_DB_DIR / \"sentence_level_db\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"document_level\",\n",
    "        \"model\": \"intfloat/multilingual-e5-large\",\n",
    "        \"db_path\": VECTOR_DB_DIR / \"document_level_db\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Embedding Model to use for chunking test\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Chunking-Methoden\n",
    "CHUNKING_METHODS = {\n",
    "    \"fixed_size\": RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=100\n",
    "    ),\n",
    "    \"sentence\": RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \";\", \":\"],\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=0\n",
    "    ),\n",
    "    \"paragraph\": RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\"],\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "}\n",
    "\n",
    "OLLAMA_MODELS = [\"llama3:8b\", \"mistral:7b-instruct\", \"phi4-mini\"]\n",
    "\n",
    "# Default chunking method for model tests\n",
    "CHUNKING_METHOD = \"paragraph\"\n",
    "CHUNKING_DB = VECTOR_DB_DIR / \"chunking_paragraph\"\n",
    "\n",
    "# Ensure Folders are created\n",
    "VECTOR_DB_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector-DB und Query Methoden definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_db(db_path, embedding_model):\n",
    "    \"\"\"Lädt eine existierende Vektordatenbank\"\"\"\n",
    "    print(f\"Lade Vector DB aus {db_path}\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "    db = Chroma(\n",
    "        persist_directory=str(db_path),\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    return db\n",
    "\n",
    "def query_ollama(prompt, model):\n",
    "    # Create a results directory if it doesn't exist\n",
    "    results_dir = BASE_DIR / \"results\"\n",
    "    results_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    output_file = results_dir / f\"query_result_{model.replace(':', '_')}.txt\"\n",
    "    \n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/chat\",\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    result = response.json()\n",
    "    \n",
    "    # Save to file instead of printing\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(result, indent=2))\n",
    "    \n",
    "    return result\n",
    "  \n",
    "def search_and_query_llm(db_path, query, embedding_model, model=OLLAMA_MODEL):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nSuche in {db_path} nach: '{query}'\")\n",
    "    \n",
    "    db = load_vector_db(db_path, embedding_model)\n",
    "    docs = db.similarity_search(query, k=3)\n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    prompt = f\"\"\"Basierend auf dem folgenden Kontext, beantworte die Frage.\n",
    "\n",
    "Kontext:\n",
    "{context}\n",
    "\n",
    "Frage: {query}\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = query_ollama(prompt, model)\n",
    "    antwortzeit = time.time() - start_time\n",
    "    content = response['message']['content']\n",
    "    \n",
    "    return {\n",
    "        \"search_time\": search_time,\n",
    "        \"answer_time\": antwortzeit,\n",
    "        \"total_time\": search_time + antwortzeit,\n",
    "        \"answer\": content\n",
    "    }\n",
    "\n",
    "\n",
    "def create_vector_db(chunks, model_path, db_path):\n",
    "    start_time = time.time()\n",
    "    memory_before = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    print(f\"\\nErstelle Vector DB mit {model_path}\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_path)\n",
    "    \n",
    "    db = Chroma.from_documents(\n",
    "        chunks,\n",
    "        embeddings,\n",
    "        persist_directory=str(db_path)\n",
    "    )\n",
    "    \n",
    "    memory_after = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "    memory_usage = memory_after - memory_before\n",
    "    creation_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Vector DB in {db_path} gespeichert\")\n",
    "    print(f\"Erstellungsdauer: {creation_time:.2f} Sekunden\")\n",
    "    print(f\"Speicherverbrauch: {memory_usage:.2f} MB\")\n",
    "    \n",
    "    return db, {\n",
    "        \"db_path\": db_path,\n",
    "        \"chunks\": len(chunks),\n",
    "        \"creation_time\": creation_time,\n",
    "        \"memory_usage\": memory_usage\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verzeichnis Struktur einlesen und laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF-Dateien: 0\n",
      "Geladen: 10 Dokumente\n"
     ]
    }
   ],
   "source": [
    "def load_documents(directory: Path):\n",
    "    documents = []\n",
    "    for file_path in directory.glob(\"*.txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        doc = Document(page_content=text, metadata={\"source\": file_path.name})\n",
    "        documents.append(doc)\n",
    "    print(f\"Geladen: {len(documents)} Dokumente\")\n",
    "    return documents\n",
    "\n",
    "for dir_path in [BASE_DIR, IMPORT_DIR, PROCESSED_DIR, INPUT_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "def list_pdf_files(directory):\n",
    "    return [f for f in directory.glob(\"*.pdf\")]\n",
    "  \n",
    "pdf_files = list_pdf_files(IMPORT_DIR)\n",
    "\n",
    "print(f\"PDF-Dateien: {len(pdf_files)}\")\n",
    "for pdf in pdf_files:\n",
    "    print(f\" - {pdf.name}\")\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text_content = []\n",
    "    \n",
    "    for page_num in range(len(document)):\n",
    "        page = document[page_num]\n",
    "        text = page.get_text()\n",
    "        \n",
    "        text = re.sub(r'\\s+', ' ', text) \n",
    "        text = text.strip()\n",
    "        \n",
    "        if text:\n",
    "            text_content.append(f\"--- Seite {page_num + 1} ---\\n{text}\")\n",
    "    \n",
    "    document.close()\n",
    "    \n",
    "    processed_text = \"\\n\\n\".join(text_content)\n",
    "    \n",
    "    processed_text = re.sub(r'([.!?])\\s*(\\w)', r'\\1\\n\\2', processed_text)  # Satzenden mit Zeilenumbrüchen\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "processed_files = []\n",
    "\n",
    "for pdf_path in pdf_files:\n",
    "    print(f\"Verarbeite: {pdf_path.name}\")\n",
    "    \n",
    "    processed_text = process_pdf(pdf_path)\n",
    "    \n",
    "    output_file = INPUT_DIR / f\"{pdf_path.stem}.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(processed_text)\n",
    "    \n",
    "    target_path = PROCESSED_DIR / pdf_path.name\n",
    "    shutil.move(pdf_path, target_path)\n",
    "    \n",
    "    processed_files.append({\n",
    "        \"original_file\": pdf_path.name,\n",
    "        \"processed_file\": output_file.name,\n",
    "        \"size_kb\": round(output_file.stat().st_size / 1024, 2)\n",
    "    })\n",
    "\n",
    "\n",
    "documents = load_documents(INPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung der Content Embeddings \n",
    "\n",
    "### Leitfragen zur Bewertung\n",
    "\n",
    "Hier wurden die PDFs manuell analysiert um einen geeignete Query zu finden um die Qualität des Outputs zu prüfen.  \n",
    "Da das Sprachmodell von z.B. phi4 auch schon viel Basis-Wissen ohne Knowledgebase hat, muss die Frage so formuliert werden, dass es nur mithilfe der Dokumente beantwortet werden kann.\n",
    "\n",
    "> Which callback function is called during training?  \n",
    "Erwartete Antwort: ModelCheckpoint\n",
    "\n",
    "### Ausgewählte Embeddings\n",
    "\n",
    "- word_level\n",
    "- sentence_level\n",
    "- document_level\n",
    "\n",
    "### Vector DB für Embedding erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumente in 222 Chunks aufgeteilt\n",
      "\n",
      "Erstelle Vector DB mit sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\knowledgebase\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB in knowledge-base\\vector-stores\\word_level_db gespeichert\n",
      "Erstellungsdauer: 58.55 Sekunden\n",
      "Speicherverbrauch: 615.20 MB\n",
      "\n",
      "Erstelle Vector DB mit sentence-transformers/all-mpnet-base-v2\n",
      "Vector DB in knowledge-base\\vector-stores\\sentence_level_db gespeichert\n",
      "Erstellungsdauer: 120.61 Sekunden\n",
      "Speicherverbrauch: 848.27 MB\n",
      "\n",
      "Erstelle Vector DB mit intfloat/multilingual-e5-large\n",
      "Vector DB in knowledge-base\\vector-stores\\document_level_db gespeichert\n",
      "Erstellungsdauer: 462.88 Sekunden\n",
      "Speicherverbrauch: 1407.14 MB\n"
     ]
    }
   ],
   "source": [
    "# Dokumente in Chunks aufteilen\n",
    "embedding_stats = {}\n",
    "\n",
    "def prepare_chunks(documents):\n",
    "    max_chunk_size = min(CONTEXT_WINDOW // 2, TOKEN_LIMIT)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_chunk_size,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Dokumente in {len(chunks)} Chunks aufgeteilt\")\n",
    "    return chunks\n",
    "\n",
    "chunks = prepare_chunks(documents)\n",
    "\n",
    "for config in EMBEDDING_CONFIGS:\n",
    "    name = config[\"name\"]\n",
    "    _, stats = create_vector_db(chunks, config[\"model\"], config[\"db_path\"])\n",
    "    embedding_stats[name] = stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluierung der Embedding Vector DBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Vergleichstest aller Vector DBs ===\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\word_level_db nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\word_level_db\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\sentence_level_db nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\sentence_level_db\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\document_level_db nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\document_level_db\n",
      "\n",
      "=== Zusammenfassung ===\n",
      "Embedding-Modell | Suchzeit (s) | Antwortzeit (s) | Gesamtzeit (s)\n",
      "-----------------------------------------------------------------\n",
      "word_level      | 2.7418 | 143.33 | 146.08\n",
      "sentence_level  | 2.4542 | 160.31 | 162.76\n",
      "document_level  | 4.9877 | 271.82 | 276.81\n"
     ]
    }
   ],
   "source": [
    "# Alle drei DBs mit der gleichen Frage testen\n",
    "print(\"\\n=== Vergleichstest aller Vector DBs ===\")\n",
    "\n",
    "embedding_results = {}\n",
    "\n",
    "for config in EMBEDDING_CONFIGS:\n",
    "     name = config[\"name\"]\n",
    "     result = search_and_query_llm(config[\"db_path\"], TEST_QUERY, config[\"model\"])\n",
    "     embedding_results[name] = result\n",
    "\n",
    "# Zusammenfassung der Ergebnisse\n",
    "print(\"\\n=== Zusammenfassung ===\")\n",
    "print(\"Embedding-Modell | Suchzeit (s) | Antwortzeit (s) | Gesamtzeit (s)\")\n",
    "print(\"-\" * 65)\n",
    "for model, result in embedding_results.items():\n",
    "    print(f\"{model:15} | {result['search_time']:.4f} | {result['answer_time']:.2f} | {result['total_time']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung des Chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Chunking-Methode: fixed_size ===\n",
      "Chunks erstellt: 759\n",
      "\n",
      "Erstelle Vector DB mit sentence-transformers/all-mpnet-base-v2\n",
      "Vector DB in knowledge-base\\vector-stores\\chunking_fixed_size gespeichert\n",
      "Erstellungsdauer: 312.23 Sekunden\n",
      "Speicherverbrauch: 675.29 MB\n",
      "\n",
      "=== Chunking-Methode: sentence ===\n",
      "Chunks erstellt: 745\n",
      "\n",
      "Erstelle Vector DB mit sentence-transformers/all-mpnet-base-v2\n",
      "Vector DB in knowledge-base\\vector-stores\\chunking_sentence gespeichert\n",
      "Erstellungsdauer: 308.94 Sekunden\n",
      "Speicherverbrauch: 314.41 MB\n",
      "\n",
      "=== Chunking-Methode: paragraph ===\n",
      "Chunks erstellt: 407\n",
      "\n",
      "Erstelle Vector DB mit sentence-transformers/all-mpnet-base-v2\n",
      "Vector DB in knowledge-base\\vector-stores\\chunking_paragraph gespeichert\n",
      "Erstellungsdauer: 216.63 Sekunden\n",
      "Speicherverbrauch: 358.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Chunking-Methoden testen\n",
    "chunking_stats = {}\n",
    "\n",
    "\n",
    "for method_name, splitter in CHUNKING_METHODS.items():\n",
    "    print(f\"\\n=== Chunking-Methode: {method_name} ===\")\n",
    "    \n",
    "    # Dokumente in Chunks aufteilen\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    print(f\"Chunks erstellt: {len(chunks)}\")\n",
    "    \n",
    "    # Vector-DB erstellen mit bestehender Methode\n",
    "    db, stats = create_vector_db(chunks, EMBEDDING_MODEL,  VECTOR_DB_DIR / f\"chunking_{method_name}\")\n",
    "    chunking_stats[method_name] = stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluierung Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suche in knowledge-base\\vector-stores\\chunking_fixed_size nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\chunking_fixed_size\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\chunking_sentence nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\chunking_sentence\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\chunking_paragraph nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\chunking_paragraph\n",
      "\n",
      "=== Zusammenfassung der Chunking-Methoden ===\n",
      "Methode        | Suchzeit (s) | Antwortzeit (s) | Gesamtzeit (s)\n",
      "-----------------------------------------------------------------\n",
      "fixed_size      | 12.4376 | 29.81 | 42.25\n",
      "sentence        | 2.7703 | 23.92 | 26.69\n",
      "paragraph       | 2.4173 | 35.82 | 38.24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chunking_results  = {}\n",
    "for method_name, splitter in CHUNKING_METHODS.items():\n",
    "    # Mit LLM testen\n",
    "    chunking_results [method_name] = search_and_query_llm(VECTOR_DB_DIR / f\"chunking_{method_name}\", TEST_QUERY, EMBEDDING_MODEL)\n",
    "        \n",
    "# Zusammenfassung\n",
    "print(\"\\n=== Zusammenfassung der Chunking-Methoden ===\")\n",
    "print(\"Methode        | Suchzeit (s) | Antwortzeit (s) | Gesamtzeit (s)\")\n",
    "print(\"-\" * 65)\n",
    "for method, result in chunking_results.items():\n",
    "    print(f\"{method:15} | {result['search_time']:.4f} | {result['answer_time']:.2f} | {result['total_time']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verschiedene Modelle Testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Vergleich der Sprachmodelle ===\n",
      "\n",
      "Testing model: llama3:8b\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\chunking_paragraph nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\chunking_paragraph\n",
      "Model: llama3:8b\n",
      "Accuracy: ✓\n",
      "Answer time: 16.48s\n",
      "Total time: 19.14s\n",
      "\n",
      "Testing model: mistral:7b-instruct\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\chunking_paragraph nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\chunking_paragraph\n",
      "Model: mistral:7b-instruct\n",
      "Accuracy: ✓\n",
      "Answer time: 92.51s\n",
      "Total time: 95.68s\n",
      "\n",
      "Testing model: phi4-mini\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\chunking_paragraph nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\chunking_paragraph\n",
      "Model: phi4-mini\n",
      "Accuracy: ✓\n",
      "Answer time: 41.62s\n",
      "Total time: 45.77s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Vergleich der Sprachmodelle ===\")\n",
    "model_results = {}\n",
    "\n",
    "for model in OLLAMA_MODELS:\n",
    "    print(f\"\\nTesting model: {model}\")\n",
    "    result = search_and_query_llm(CHUNKING_DB, TEST_QUERY, EMBEDDING_MODEL, model)\n",
    "    \n",
    "    accuracy = EXPECTED_ANSWER.lower() in result[\"answer\"].lower()\n",
    "    model_results[model] = {\n",
    "        \"answer_time\": result[\"answer_time\"],\n",
    "        \"total_time\": result[\"total_time\"],\n",
    "        \"accuracy\": accuracy,\n",
    "        \"answer\": result[\"answer\"]\n",
    "    }\n",
    "    \n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Accuracy: {'✓' if accuracy else '✗'}\")\n",
    "    print(f\"Answer time: {result['answer_time']:.2f}s\")\n",
    "    print(f\"Total time: {result['total_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zusammenfassung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ZUSAMMENFASSUNG ALLER ERGEBNISSE ===\n",
      "\n",
      "1. EMBEDDING MODELLE\n",
      "Embedding-Modell | Accuracy | Memory (MB) | Suchzeit (s) | Gesamtzeit (s)\n",
      "---------------------------------------------------------------------------\n",
      "word_level      | ✗        |     615.20 |      2.7418 |        146.08\n",
      "sentence_level  | ✓        |     848.27 |      2.4542 |        162.76\n",
      "document_level  | ✓        |    1407.14 |      4.9877 |        276.81\n",
      "\n",
      "2. CHUNKING METHODEN\n",
      "Methode      | Chunks | Accuracy | Memory (MB) | Suchzeit (s) | Gesamtzeit (s)\n",
      "--------------------------------------------------------------------------------\n",
      "fixed_size   |    759 | ✓        |     675.29 |     12.4376 |         42.25\n",
      "sentence     |    745 | ✓        |     314.41 |      2.7703 |         26.69\n",
      "paragraph    |    407 | ✓        |     358.00 |      2.4173 |         38.24\n",
      "\n",
      "3. LLM MODELLE\n",
      "Modell        | Accuracy | Antwortzeit (s) | Gesamtzeit (s)\n",
      "------------------------------------------------------------\n",
      "llama3:8b     | ✓        |          16.48 |         19.14\n",
      "mistral:7b-instruct | ✓        |          92.51 |         95.68\n",
      "phi4-mini     | ✓        |          41.62 |         45.77\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation code block\n",
    "print(\"\\n=== ZUSAMMENFASSUNG ALLER ERGEBNISSE ===\\n\")\n",
    "\n",
    "# Embedding comparison\n",
    "print(\"1. EMBEDDING MODELLE\")\n",
    "print(\"Embedding-Modell | Accuracy | Memory (MB) | Suchzeit (s) | Gesamtzeit (s)\")\n",
    "print(\"-\" * 75)\n",
    "for config in EMBEDDING_CONFIGS:\n",
    "    model_name = config[\"name\"]\n",
    "    result = embedding_results[model_name]  # From your embedding tests\n",
    "    memory = embedding_stats[model_name][\"memory_usage\"]  # Collect during vector DB creation\n",
    "    accuracy = \"✓\" if EXPECTED_ANSWER.lower() in result[\"answer\"].lower() else \"✗\"\n",
    "    print(f\"{model_name:15} | {accuracy:8} | {memory:10.2f} | {result['search_time']:11.4f} | {result['total_time']:13.2f}\")\n",
    "\n",
    "# Chunking comparison\n",
    "print(\"\\n2. CHUNKING METHODEN\")\n",
    "print(\"Methode      | Chunks | Accuracy | Memory (MB) | Suchzeit (s) | Gesamtzeit (s)\")\n",
    "print(\"-\" * 80)\n",
    "for method_name, result in chunking_results.items():  # From your chunking tests\n",
    "    chunks = chunking_stats[method_name][\"chunks\"]\n",
    "    memory = chunking_stats[method_name][\"memory_usage\"]\n",
    "    accuracy = \"✓\" if EXPECTED_ANSWER.lower() in result[\"answer\"].lower() else \"✗\"\n",
    "    print(f\"{method_name:12} | {chunks:6} | {accuracy:8} | {memory:10.2f} | {result['search_time']:11.4f} | {result['total_time']:13.2f}\")\n",
    "\n",
    "# Model comparison \n",
    "print(\"\\n3. LLM MODELLE\")\n",
    "print(\"Modell        | Accuracy | Antwortzeit (s) | Gesamtzeit (s)\")\n",
    "print(\"-\" * 60)\n",
    "for model, result in model_results.items():\n",
    "    accuracy = \"✓\" if result[\"accuracy\"] else \"✗\"\n",
    "    print(f\"{model:13} | {accuracy:8} | {result['answer_time']:14.2f} | {result['total_time']:13.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
