{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Base\n",
    "\n",
    "## Auswahl der Sprachmodelle \n",
    "\n",
    "### llama3:8b\n",
    "- **Leistung**: Starke Performance für RAG-Anwendungen mit guter Kontextverarbeitung\n",
    "- **Größe**: Mit 8B Parametern effizient auf Consumer-Hardware lauffähig\n",
    "- **Lizenz**: Permissive Lizenz ermöglicht kommerzielle Nutzung\n",
    "- **Aktualität**: Neues Modell mit modernem Trainingsdatenset und verbesserter Instruction-Following-Fähigkeit\n",
    "\n",
    "### mistral:7b-instruct\n",
    "- **Effizienz**: Ausgezeichnetes Leistungs-Größen-Verhältnis\n",
    "- **Spezialisierung**: Optimiert für Instruction-Following und Kontextverständnis\n",
    "- **Architektur**: Gruppenweise Rotation der Aufmerksamkeit für verbesserte Verarbeitung langer Dokumente\n",
    "- **Community-Support**: Breite Nutzerbasis und dokumentierte Anwendungsfälle für RAG\n",
    "\n",
    "### phi4-mini\n",
    "- **Ressourcenschonung**: Kleines Modell (3.8B) für Systeme mit begrenzten Ressourcen\n",
    "- **Effizienz**: Hervorragende Leistung trotz geringer Größe\n",
    "- **Antwortqualität**: Gute Formulierungsfähigkeit bei unternehmensbezogenen Inhalten\n",
    "- **Kompatibilität**: Geringer VRAM-Bedarf macht es auf verschiedenen Systemen einsetzbar\n",
    "\n",
    "Dieser Mix bietet eine gute Balance zwischen Performance, Ressourcenbedarf und verschiedenen Architekturen für einen aussagekräftigen Vergleich.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Einrichtung der Knowledge Base\n",
    "\n",
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF-Dateien: 10\n",
      " - 1-Challenges_and_Reliability_of_Predictive_Maintenance.pdf\n",
      " - 2-1478_unsupervised_model_selection_for unsupervised anomaly detection.pdf\n",
      " - 3-Anomaly_Detection_in_Renewable_Energy_Big_Data_Usi.pdf\n",
      " - 4-Anomaly Detection and Diagnosis In Manufacturing Systems A Comparative Study Of Statistical, Machine Learning And Deep Learning Techniques.pdf\n",
      " - 5-comparative study of deep learning autoencoders for vibration anomaly detection in manufacturing equipement.pdf\n",
      "Verarbeite: 1-Challenges_and_Reliability_of_Predictive_Maintenance.pdf\n",
      "Verarbeite: 2-1478_unsupervised_model_selection_for unsupervised anomaly detection.pdf\n",
      "Verarbeite: 3-Anomaly_Detection_in_Renewable_Energy_Big_Data_Usi.pdf\n",
      "Verarbeite: 4-Anomaly Detection and Diagnosis In Manufacturing Systems A Comparative Study Of Statistical, Machine Learning And Deep Learning Techniques.pdf\n",
      "Verarbeite: 5-comparative study of deep learning autoencoders for vibration anomaly detection in manufacturing equipement.pdf\n",
      "Verarbeite: 6-two step ML approach for pdm and anomaly detection in enviro sensor sys.pdf\n",
      "Verarbeite: 7-an ensemble learning based alert trigger system for pdm of assets.pdf\n",
      "Verarbeite: 8-Teubert_An Analysis of Barriers Preventing the Widespread Adoption of Predictive and Prescriptive Maintenance in Aviation (1).pdf#_~_t.pdf\n",
      "Verarbeite: 9-PATE Proximity Aware Time series anomaly Evaluation.pdf\n",
      "Verarbeite: UnsuperFrameworkPdM.pdf\n",
      "Geladen: 10 Dokumente\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import fitz\n",
    "import re\n",
    "from typing import List\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from langchain.schema import Document\n",
    "import time\n",
    "import psutil\n",
    "import requests\n",
    "\n",
    "# Folder structure\n",
    "BASE_DIR = Path(\"knowledge-base\")\n",
    "IMPORT_DIR = BASE_DIR / \"import\"\n",
    "PROCESSED_DIR = BASE_DIR / \"processed\"\n",
    "INPUT_DIR = BASE_DIR / \"embeddings-ready\"\n",
    "VECTOR_DB_DIR = BASE_DIR / \"vector-stores\"\n",
    "\n",
    "\n",
    "# Query Options\n",
    "CONTEXT_WINDOW = 8192\n",
    "TOKEN_LIMIT = 4096\n",
    "OLLAMA_MODEL = \"phi4-mini\"  # Ollama-Modell\n",
    "TEST_QUERY = \"Which callback function is called during training?\"\n",
    "EXPECTED_ANSWER = \"ModelCheckpoint\"\n",
    "\n",
    "# Define Embedding Configs\n",
    "EMBEDDING_CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"word_level\",\n",
    "        \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"db_path\": VECTOR_DB_DIR / \"word_level_db\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"sentence_level\",\n",
    "        \"model\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        \"db_path\": VECTOR_DB_DIR / \"sentence_level_db\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"document_level\",\n",
    "        \"model\": \"intfloat/multilingual-e5-large\",\n",
    "        \"db_path\": VECTOR_DB_DIR / \"document_level_db\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Embedding Model to use for chunking test\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Chunking-Methoden\n",
    "CHUNKING_METHODS = {\n",
    "    \"fixed_size\": RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=100\n",
    "    ),\n",
    "    \"sentence\": RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \";\", \":\"],\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=0\n",
    "    ),\n",
    "    \"paragraph\": RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\"],\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "}\n",
    "\n",
    "# Ensure Folders are created\n",
    "VECTOR_DB_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector-DB und Query Methoden definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_db(db_path, embedding_model):\n",
    "    \"\"\"Lädt eine existierende Vektordatenbank\"\"\"\n",
    "    print(f\"Lade Vector DB aus {db_path}\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "    db = Chroma(\n",
    "        persist_directory=str(db_path),\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    return db\n",
    "\n",
    "def query_ollama(prompt, model=OLLAMA_MODEL):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/chat\",\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    return response.json()\n",
    "  \n",
    "def search_and_query_llm(db_path, query, embedding_model):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nSuche in {db_path} nach: '{query}'\")\n",
    "    \n",
    "    db = load_vector_db(db_path, embedding_model)\n",
    "    docs = db.similarity_search(query, k=3)\n",
    "    suchzeit = time.time() - start_time\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    prompt = f\"\"\"Basierend auf dem folgenden Kontext, beantworte die Frage.\n",
    "\n",
    "Kontext:\n",
    "{context}\n",
    "\n",
    "Frage: {query}\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = query_ollama(prompt)\n",
    "    antwortzeit = time.time() - start_time\n",
    "    content = response['message']['content']\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"FRAGE:\")\n",
    "    print(\"-\"*60)\n",
    "    print(query)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"ANTWORT:\")\n",
    "    print(\"-\"*60)\n",
    "    print(content)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"METADATEN:\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"Suchzeit:       {suchzeit:.4f} Sekunden\")\n",
    "    print(f\"LLM-Antwortzeit: {antwortzeit:.2f} Sekunden\")\n",
    "    print(f\"Gesamtzeit:     {suchzeit + antwortzeit:.2f} Sekunden\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"suchzeit\": suchzeit,\n",
    "        \"antwortzeit\": antwortzeit,\n",
    "        \"gesamtzeit\": suchzeit + antwortzeit,\n",
    "        \"antwort\": content\n",
    "    }\n",
    "\n",
    "\n",
    "def create_vector_db(chunks, model_path, db_path):\n",
    "    start_time = time.time()\n",
    "    memory_before = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    print(f\"\\nErstelle Vector DB mit {model_path}\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_path)\n",
    "    \n",
    "    db = Chroma.from_documents(\n",
    "        chunks,\n",
    "        embeddings,\n",
    "        persist_directory=str(db_path)\n",
    "    )\n",
    "    \n",
    "    memory_after = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "    erstellungszeit = time.time() - start_time\n",
    "    \n",
    "    print(f\"Vector DB in {db_path} gespeichert\")\n",
    "    print(f\"Erstellungsdauer: {erstellungszeit:.2f} Sekunden\")\n",
    "    print(f\"Speicherverbrauch: {memory_after - memory_before:.2f} MB\")\n",
    "    \n",
    "    return db, db_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verzeichnis Struktur einlesen und laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(directory: Path):\n",
    "    documents = []\n",
    "    for file_path in directory.glob(\"*.txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        doc = Document(page_content=text, metadata={\"source\": file_path.name})\n",
    "        documents.append(doc)\n",
    "    print(f\"Geladen: {len(documents)} Dokumente\")\n",
    "    return documents\n",
    "\n",
    "for dir_path in [BASE_DIR, IMPORT_DIR, PROCESSED_DIR, INPUT_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "def list_pdf_files(directory):\n",
    "    return [f for f in directory.glob(\"*.pdf\")]\n",
    "  \n",
    "pdf_files = list_pdf_files(IMPORT_DIR)\n",
    "\n",
    "print(f\"PDF-Dateien: {len(pdf_files)}\")\n",
    "for pdf in pdf_files:\n",
    "    print(f\" - {pdf.name}\")\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text_content = []\n",
    "    \n",
    "    for page_num in range(len(document)):\n",
    "        page = document[page_num]\n",
    "        text = page.get_text()\n",
    "        \n",
    "        text = re.sub(r'\\s+', ' ', text) \n",
    "        text = text.strip()\n",
    "        \n",
    "        if text:\n",
    "            text_content.append(f\"--- Seite {page_num + 1} ---\\n{text}\")\n",
    "    \n",
    "    document.close()\n",
    "    \n",
    "    processed_text = \"\\n\\n\".join(text_content)\n",
    "    \n",
    "    processed_text = re.sub(r'([.!?])\\s*(\\w)', r'\\1\\n\\2', processed_text)  # Satzenden mit Zeilenumbrüchen\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "processed_files = []\n",
    "\n",
    "for pdf_path in pdf_files:\n",
    "    print(f\"Verarbeite: {pdf_path.name}\")\n",
    "    \n",
    "    processed_text = process_pdf(pdf_path)\n",
    "    \n",
    "    output_file = INPUT_DIR / f\"{pdf_path.stem}.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(processed_text)\n",
    "    \n",
    "    target_path = PROCESSED_DIR / pdf_path.name\n",
    "    shutil.move(pdf_path, target_path)\n",
    "    \n",
    "    processed_files.append({\n",
    "        \"original_file\": pdf_path.name,\n",
    "        \"processed_file\": output_file.name,\n",
    "        \"size_kb\": round(output_file.stat().st_size / 1024, 2)\n",
    "    })\n",
    "\n",
    "\n",
    "documents = load_documents(INPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung der Content Embeddings \n",
    "\n",
    "### Leitfragen zur Bewertung\n",
    "\n",
    "Hier wurden die PDFs manuell analysiert um einen geeignete Query zu finden um die Qualität des Outputs zu prüfen.  \n",
    "Da das Sprachmodell von z.B. phi4 auch schon viel Basis-Wissen ohne Knowledgebase hat, muss die Frage so formuliert werden, dass es nur mithilfe der Dokumente beantwortet werden kann.\n",
    "\n",
    "> Which callback function is called during training?\n",
    "Erwartete Antwort: ModelCheckpoint\n",
    "\n",
    "### Ausgewählte Embeddings\n",
    "\n",
    "- word_level\n",
    "- sentence_level\n",
    "- document_level\n",
    "\n",
    "### Vector DB für Embedding erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumente in 222 Chunks aufgeteilt\n",
      "\n",
      "Erstelle Vector DB mit sentence-transformers/all-MiniLM-L6-v2\n",
      "Vector DB in knowledge-base\\vector-stores\\word_level_db gespeichert\n",
      "Erstellungsdauer: 13.91 Sekunden\n",
      "Speicherverbrauch: 258.41 MB\n",
      "\n",
      "Erstelle Vector DB mit sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\knowledgebase\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Martin\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB in knowledge-base\\vector-stores\\sentence_level_db gespeichert\n",
      "Erstellungsdauer: 144.09 Sekunden\n",
      "Speicherverbrauch: 782.60 MB\n",
      "\n",
      "Erstelle Vector DB mit intfloat/multilingual-e5-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\knowledgebase\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Martin\\.cache\\huggingface\\hub\\models--intfloat--multilingual-e5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB in knowledge-base\\vector-stores\\document_level_db gespeichert\n",
      "Erstellungsdauer: 545.12 Sekunden\n",
      "Speicherverbrauch: 1415.49 MB\n"
     ]
    }
   ],
   "source": [
    "# Dokumente in Chunks aufteilen\n",
    "def prepare_chunks(documents):\n",
    "    max_chunk_size = min(CONTEXT_WINDOW // 2, TOKEN_LIMIT)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_chunk_size,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Dokumente in {len(chunks)} Chunks aufgeteilt\")\n",
    "    return chunks\n",
    "\n",
    "chunks = prepare_chunks(documents)\n",
    "\n",
    "for config in EMBEDDING_CONFIGS:\n",
    "    create_vector_db(chunks, config[\"model\"], config[\"db_path\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluierung der Embedding Vector DBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Vergleichstest aller Vector DBs ===\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\word_level_db nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\word_level_db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\AppData\\Local\\Temp\\ipykernel_23316\\3619819193.py:133: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FRAGE:\n",
      "------------------------------------------------------------\n",
      "Which callback function is called during training?\n",
      "============================================================\n",
      "ANTWORT:\n",
      "------------------------------------------------------------\n",
      "The provided context does not explicitly mention any Python code that includes a callback function being used in an autoencoder or deep-learning models for anomaly detection. Callback functions are typically found within machine learning frameworks such as TensorFlow/Keras, and they can be set up to execute certain actions at various stages of the model training process (e.g., after each epoch).\n",
      "\n",
      "Common types of callbacks include:\n",
      "\n",
      "1. EarlyStopping: Stops training when a monitored metric has stopped improving.\n",
      "2. ModelCheckpoint: Saves the best-performing model during the entire history or periodically as specified by 'save_best_only' and 'period'.\n",
      "3. ReduceLROnPlateau: Decreases learning rate once an epoch with no improvement in performance is detected for epochs.\n",
      "4. TensorBoard (as hinted at in Figure 9a–d): Provides visualization of training process metrics.\n",
      "\n",
      "Without specific code examples from the provided context, I cannot determine which callback function was called during your model's training sessions as it would depend on how you have structured and implemented these callbacks within a Python script or Jupyter notebook. If this information is needed for an actual implementation you're working with in Electronics 2024 (or any other project), please provide the relevant code snippet so I can assist further!\n",
      "============================================================\n",
      "METADATEN:\n",
      "------------------------------------------------------------\n",
      "Suchzeit:       12.0468 Sekunden\n",
      "LLM-Antwortzeit: 155.31 Sekunden\n",
      "Gesamtzeit:     167.36 Sekunden\n",
      "============================================================\n",
      "\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\sentence_level_db nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\sentence_level_db\n",
      "\n",
      "============================================================\n",
      "FRAGE:\n",
      "------------------------------------------------------------\n",
      "Which callback function is called during training?\n",
      "============================================================\n",
      "ANTWORT:\n",
      "------------------------------------------------------------\n",
      "The context provided does not explicitly mention any specific callback functions used in the proposed methodology for anomaly detection. Callback functions are typically associated with machine learning libraries such as TensorFlow or PyTorch when they train models and monitor certain conditions to execute custom code.\n",
      "\n",
      "For example, if you were using a neural network library like Keras (which is part of TensorFlow), common callbacks might include:\n",
      "\n",
      "- `ModelCheckpoint`: Saves the model after every epoch.\n",
      "- `EarlyStopping`: Stops training when there’s no improvement on validation loss for 'patience' number of epochs (`monitor='val_loss', patience=10`).\n",
      "- `ReduceLROnPlateau`: Reduces learning rate once a metric has stopped improving.\n",
      "\n",
      "Since none of these are mentioned in the provided context, we cannot determine which callback function is called during training. Callback functions would need to be specified or defined within your code for this question's answer.\n",
      "============================================================\n",
      "METADATEN:\n",
      "------------------------------------------------------------\n",
      "Suchzeit:       2.6318 Sekunden\n",
      "LLM-Antwortzeit: 148.79 Sekunden\n",
      "Gesamtzeit:     151.43 Sekunden\n",
      "============================================================\n",
      "\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\document_level_db nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\document_level_db\n",
      "\n",
      "============================================================\n",
      "FRAGE:\n",
      "------------------------------------------------------------\n",
      "Which callback function is called during training?\n",
      "============================================================\n",
      "ANTWORT:\n",
      "------------------------------------------------------------\n",
      "In the provided context from a publication related to anomaly detection using deep learning techniques for industrial applications (specifically mentioning Isolation Forests and Deep Autoencoders), there isn't explicit information about which specific machine learning framework or library they are referring to. However, in many common scenarios involving Convolutional Neural Networks (CNN) with PyTorch as an example:\n",
      "\n",
      "In the case of training a CNN using PyTorch's `torch.nn` module for anomaly detection tasks like image classification and segmentation (`UNet`, etc.), you might have callbacks implemented manually or utilizing libraries such as TensorBoard, which uses callback functions to log various metrics during model evaluation.\n",
      "\n",
      "For instance:\n",
      "- A learning rate scheduler (like ReduceLROnPlateau) can be used in conjunction with a custom function that reduces the learning rate when validation loss plateaus.\n",
      "  \n",
      "  Example usage of `ReduceLROnPlateau`:\n",
      "\n",
      "```python\n",
      "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
      "\n",
      "# Assume 'optimizer' is your chosen optimizer, e.g., Adam or SGD \n",
      "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
      "```\n",
      "\n",
      "- To log the training progress to a file using `TensorBoard`, you could create custom callbacks that are invoked at certain points during model evaluation.\n",
      "\n",
      "  Example usage of TensorBoard:\n",
      "\n",
      "```python\n",
      "from torch.utils.tensorboard import SummaryWriter\n",
      "\n",
      "writer = SummaryWriter()\n",
      "\n",
      "# During your validation loop or after each epoch:\n",
      "writer.add_scalar('Loss/train', loss, global_step)\n",
      "```\n",
      "\n",
      "- Some deep learning libraries also allow for the use of built-in callback functions that can be used to monitor and log training progress. However, these are more common in Keras (a high-level neural networks API) rather than PyTorch.\n",
      "\n",
      "For custom callbacks during training using a framework like TensorFlow or another library:\n",
      "- You would typically define an `__init__` method where you initialize any variables that the callback will need to use.\n",
      "- Then there is usually one main method, often named something akin to `_on_epoch_end`, which includes logic for what should happen at certain points during training (e.g., end of each epoch).\n",
      "\n",
      "Since this context does not provide information on whether they are using TensorFlow/Keras or PyTorch specifically and lacks explicit callback functions being mentioned in the text provided. If you can point me toward a particular section where callbacks might be discussed, I could give more precise guidance related to that framework's usage.\n",
      "============================================================\n",
      "METADATEN:\n",
      "------------------------------------------------------------\n",
      "Suchzeit:       5.3090 Sekunden\n",
      "LLM-Antwortzeit: 279.75 Sekunden\n",
      "Gesamtzeit:     285.06 Sekunden\n",
      "============================================================\n",
      "\n",
      "\n",
      "=== Zusammenfassung ===\n",
      "Embedding-Modell | Suchzeit (s) | Antwortzeit (s) | Gesamtzeit (s)\n",
      "-----------------------------------------------------------------\n",
      "word_level      | 12.0468 | 155.31 | 167.36\n",
      "sentence_level  | 2.6318 | 148.79 | 151.43\n",
      "document_level  | 5.3090 | 279.75 | 285.06\n"
     ]
    }
   ],
   "source": [
    "# Alle drei DBs mit der gleichen Frage testen\n",
    "print(\"\\n=== Vergleichstest aller Vector DBs ===\")\n",
    "\n",
    "results = {}\n",
    "for config in EMBEDDING_CONFIGS:\n",
    "    results[config[\"name\"]] = search_and_query_llm(config[\"db_path\"], TEST_QUERY, config[\"model\"])\n",
    "\n",
    "# Zusammenfassung der Ergebnisse\n",
    "print(\"\\n=== Zusammenfassung ===\")\n",
    "print(\"Embedding-Modell | Suchzeit (s) | Antwortzeit (s) | Gesamtzeit (s)\")\n",
    "print(\"-\" * 65)\n",
    "for model, result in results.items():\n",
    "    print(f\"{model:15} | {result['suchzeit']:.4f} | {result['antwortzeit']:.2f} | {result['gesamtzeit']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung des Chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Chunking-Methode: fixed_size ===\n",
      "Chunks erstellt: 759\n",
      "\n",
      "Erstelle Vector DB mit sentence-transformers/all-mpnet-base-v2\n",
      "Vector DB in chunking_fixed_size gespeichert\n",
      "Erstellungsdauer: 345.41 Sekunden\n",
      "Speicherverbrauch: 640.40 MB\n",
      "\n",
      "=== Chunking-Methode: sentence ===\n",
      "Chunks erstellt: 745\n",
      "\n",
      "Erstelle Vector DB mit sentence-transformers/all-mpnet-base-v2\n",
      "Vector DB in chunking_sentence gespeichert\n",
      "Erstellungsdauer: 301.90 Sekunden\n",
      "Speicherverbrauch: 368.23 MB\n",
      "\n",
      "=== Chunking-Methode: paragraph ===\n",
      "Chunks erstellt: 407\n",
      "\n",
      "Erstelle Vector DB mit sentence-transformers/all-mpnet-base-v2\n",
      "Vector DB in chunking_paragraph gespeichert\n",
      "Erstellungsdauer: 209.86 Sekunden\n",
      "Speicherverbrauch: 416.54 MB\n"
     ]
    }
   ],
   "source": [
    "# Chunking-Methoden testen\n",
    "for method_name, splitter in CHUNKING_METHODS.items():\n",
    "    print(f\"\\n=== Chunking-Methode: {method_name} ===\")\n",
    "    \n",
    "    # Dokumente in Chunks aufteilen\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    print(f\"Chunks erstellt: {len(chunks)}\")\n",
    "    \n",
    "    # Vector-DB erstellen mit bestehender Methode\n",
    "    db, _ = create_vector_db(chunks, EMBEDDING_MODEL,  VECTOR_DB_DIR / f\"chunking_{method_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluierung Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suche in knowledge-base\\vector-stores\\chunking_fixed_size nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\chunking_fixed_size\n",
      "\n",
      "============================================================\n",
      "FRAGE:\n",
      "------------------------------------------------------------\n",
      "Which callback function is called during training?\n",
      "============================================================\n",
      "ANTWORT:\n",
      "------------------------------------------------------------\n",
      "Im PyTorch Framework wird während des Trainingsprozesses der Callback-Funktion `backward()` an das Modell übergeben. Dies ist Teil von PyTorchs Autograd-Modul (Automatic Differentiation), welches den Gradienten im Backpropagation-Prozess berechnet.\n",
      "\n",
      "Um mehr Klarheit zu schaffen, hier ein Beispiel:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "\n",
      "# Ein einfaches Modell definieren.\n",
      "model = MyModel()\n",
      "\n",
      "# Ein Callback-Funktion für Training vorbereiten.\n",
      "def train_callback(optimizer):\n",
      "    def callback_fn(engine, batch):\n",
      "        # Der Trainingsprozess wird an der aktuellen Batch-Einheit ausgeführt.\n",
      "        \n",
      "        optimizer.zero_grad()  # Gradientenberechnung zurücksetzen\n",
      "        inputs, targets = batch\n",
      "        \n",
      "        outputs = model(inputs)  # Modell-Ausgabe berechnen\n",
      "        loss = criterion(outputs, targets)  # Verlust berechnen\n",
      "\n",
      "        # Der Gradient neu berechnen und den Optimierer aktualisieren\n",
      "        loss.backward()\n",
      "        \n",
      "        optimizer.step()  # Den Optimierer verwenden um die Parameter zu aktualisieren\n",
      "        \n",
      "        return {'loss': loss.item()}\n",
      "    \n",
      "    return callback_fn\n",
      "\n",
      "# Dann können Sie diesen Callback-Funktion mit einem Trainings-Engine wie Ignite verbinden.\n",
      "```\n",
      "\n",
      "Im obigen Code wird der `train_callback` verwendet, um während des Training-Prozesses Gradientenberechnungen durchzuführen und den Optimierer auf die Parameter zu aktualisieren. Der Schlüsselpunkt hier ist das Aufrufen von `loss.backward()`, was PyTorchs Autograd dazu veranlasst, Gradiente für alle Parameterelemente im Modell berechnen.\n",
      "\n",
      "Zusammenfassend ruft während des Trainingsprozesses in PyTorch hauptsächlich der Callback-Funktion `backward()` auf. Dies ermöglicht die automatische Berechnung und Aktualisierung von Gradienten, wodurch das Training effizienter wird.\n",
      "============================================================\n",
      "METADATEN:\n",
      "------------------------------------------------------------\n",
      "Suchzeit:       12.8485 Sekunden\n",
      "LLM-Antwortzeit: 76.32 Sekunden\n",
      "Gesamtzeit:     89.17 Sekunden\n",
      "============================================================\n",
      "\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\chunking_sentence nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\chunking_sentence\n",
      "\n",
      "============================================================\n",
      "FRAGE:\n",
      "------------------------------------------------------------\n",
      "Which callback function is called during training?\n",
      "============================================================\n",
      "ANTWORT:\n",
      "------------------------------------------------------------\n",
      "During the training of a machine learning model or neural network using TensorFlow/Keras in Python, several events occur at different stages. One specific event occurs when batches are fed into your dataset for processing and updating weights; this typically happens inside an `on_train_batch_end` callback.\n",
      "\n",
      "The corresponding function that is called after each batch has been processed during training (i.e., the end of a single iteration over one sample in every mini-batch) would be something like:\n",
      "\n",
      "```python\n",
      "def on_train_batch_end(batch, logs):\n",
      "    # Code to execute at this point with access to `batch` and other information.\n",
      "```\n",
      "\n",
      "So if you are asking about which function is triggered after processing each batch during training (not just once), it should reference an event handler within the Keras callbacks such as:\n",
      "\n",
      "- For custom logic: You might implement a callback by extending from `_Callback`.\n",
      "  \n",
      "If you're looking for built-in ones specifically, these would be invoked at appropriate times:\n",
      "- `on_train_batch_end`: after each batch is processed.\n",
      "- Other relevant events like `on_epoch_begin`, etc.\n",
      "\n",
      "For an example of the actual code in Keras:\n",
      "\n",
      "```python\n",
      "from keras.callbacks import Callback\n",
      "\n",
      "class CustomCallback(Callback):\n",
      "    def on_train_batch_end(self, batch, logs=None):\n",
      "        # Code to execute at this point with access to 'batch' and other information.\n",
      "```\n",
      "\n",
      "This would be invoked every time a new mini-batch is processed during the training phase.\n",
      "============================================================\n",
      "METADATEN:\n",
      "------------------------------------------------------------\n",
      "Suchzeit:       2.5696 Sekunden\n",
      "LLM-Antwortzeit: 56.69 Sekunden\n",
      "Gesamtzeit:     59.26 Sekunden\n",
      "============================================================\n",
      "\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\chunking_paragraph nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\chunking_paragraph\n",
      "\n",
      "============================================================\n",
      "FRAGE:\n",
      "------------------------------------------------------------\n",
      "Which callback function is called during training?\n",
      "============================================================\n",
      "ANTWORT:\n",
      "------------------------------------------------------------\n",
      "The context provided does not contain enough information to determine which specific callback functions are invoked or what conditions trigger their calls. Callback functions in machine learning frameworks like TensorFlow/Keras can be used for various purposes such as logging metrics, saving models at intervals (model checkpointing), stopping the training early if a certain condition is met ('EarlyStopping'), and others.\n",
      "\n",
      "In general terms without additional context:\n",
      "\n",
      "- `on_train_begin`: Called once before any batches are passed through your network.\n",
      "- `on_epoch_end`: Invoked after each epoch has completed. It's commonly used for logging metrics at specified intervals (e.g., every 10 epochs).\n",
      "- `on_batch_begin` / `on_batch_end`: Triggered on the start and end of a batch, respectively.\n",
      "\n",
      "For more specific callback functions or to understand when they are called during training in your particular context, you would need details like whether you're using TensorFlow/Keras (`tf.keras.callbacks.Callback`) with early stopping enabled (which uses an EarlyStopping callback) for instance.\n",
      "============================================================\n",
      "METADATEN:\n",
      "------------------------------------------------------------\n",
      "Suchzeit:       2.6560 Sekunden\n",
      "LLM-Antwortzeit: 39.29 Sekunden\n",
      "Gesamtzeit:     41.94 Sekunden\n",
      "============================================================\n",
      "\n",
      "\n",
      "=== Zusammenfassung der Chunking-Methoden ===\n",
      "Methode        | Suchzeit (s) | Antwortzeit (s) | Gesamtzeit (s)\n",
      "-----------------------------------------------------------------\n",
      "fixed_size      | 12.8485 | 76.32 | 89.17\n",
      "sentence        | 2.5696 | 56.69 | 59.26\n",
      "paragraph       | 2.6560 | 39.29 | 41.94\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = {}\n",
    "for method_name, splitter in CHUNKING_METHODS.items():\n",
    "    # Mit LLM testen\n",
    "    results[method_name] = search_and_query_llm(VECTOR_DB_DIR / f\"chunking_{method_name}\", TEST_QUERY, EMBEDDING_MODEL)\n",
    "        \n",
    "# Zusammenfassung\n",
    "print(\"\\n=== Zusammenfassung der Chunking-Methoden ===\")\n",
    "print(\"Methode        | Suchzeit (s) | Antwortzeit (s) | Gesamtzeit (s)\")\n",
    "print(\"-\" * 65)\n",
    "for method, result in results.items():\n",
    "    print(f\"{method:15} | {result['suchzeit']:.4f} | {result['antwortzeit']:.2f} | {result['gesamtzeit']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
