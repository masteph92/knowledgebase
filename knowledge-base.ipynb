{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Base\n",
    "\n",
    "## Auswahl der Sprachmodelle \n",
    "\n",
    "### Llama 3.3 (8B Version)\n",
    "- **Leistung**: Starke Performance für RAG-Anwendungen mit guter Kontextverarbeitung\n",
    "- **Größe**: Mit 8B Parametern effizient auf Consumer-Hardware lauffähig\n",
    "- **Lizenz**: Permissive Lizenz ermöglicht kommerzielle Nutzung\n",
    "- **Aktualität**: Neues Modell mit modernem Trainingsdatenset und verbesserter Instruction-Following-Fähigkeit\n",
    "\n",
    "### Mistral 7B Instruct\n",
    "- **Effizienz**: Ausgezeichnetes Leistungs-Größen-Verhältnis\n",
    "- **Spezialisierung**: Optimiert für Instruction-Following und Kontextverständnis\n",
    "- **Architektur**: Gruppenweise Rotation der Aufmerksamkeit für verbesserte Verarbeitung langer Dokumente\n",
    "- **Community-Support**: Breite Nutzerbasis und dokumentierte Anwendungsfälle für RAG\n",
    "\n",
    "### Phi-4 (Mini)\n",
    "- **Ressourcenschonung**: Kleines Modell (3.8B) für Systeme mit begrenzten Ressourcen\n",
    "- **Effizienz**: Hervorragende Leistung trotz geringer Größe\n",
    "- **Antwortqualität**: Gute Formulierungsfähigkeit bei unternehmensbezogenen Inhalten\n",
    "- **Kompatibilität**: Geringer VRAM-Bedarf macht es auf verschiedenen Systemen einsetzbar\n",
    "\n",
    "Dieser Mix bietet eine gute Balance zwischen Performance, Ressourcenbedarf und verschiedenen Architekturen für einen aussagekräftigen Vergleich.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Einrichtung der Knowledge Base\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF-Dateien: 0\n",
      "Geladen: 10 Dokumente\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import fitz\n",
    "import re\n",
    "from typing import List\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from langchain.schema import Document\n",
    "import time\n",
    "import psutil\n",
    "import requests\n",
    "\n",
    "BASE_DIR = Path(\"knowledge-base\")\n",
    "IMPORT_DIR = BASE_DIR / \"import\"\n",
    "PROCESSED_DIR = BASE_DIR / \"processed\"\n",
    "INPUT_DIR = BASE_DIR / \"embeddings-ready\"\n",
    "VECTOR_DB_DIR = BASE_DIR / \"vector-stores\"\n",
    "\n",
    "\n",
    "CONTEXT_WINDOW = 8192\n",
    "TOKEN_LIMIT = 4096\n",
    "OLLAMA_MODEL = \"phi4-mini\"  # Ollama-Modell\n",
    "TEST_QUERY = \"Which callback function is called during training?\"\n",
    "\n",
    "EMBEDDING_CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"word_level\",\n",
    "        \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"db_path\": VECTOR_DB_DIR / \"word_level_db\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"sentence_level\",\n",
    "        \"model\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        \"db_path\": VECTOR_DB_DIR / \"sentence_level_db\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"document_level\",\n",
    "        \"model\": \"intfloat/multilingual-e5-large\",\n",
    "        \"db_path\": VECTOR_DB_DIR / \"document_level_db\"\n",
    "    }\n",
    "]\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Chunking-Methoden\n",
    "CHUNKING_METHODS = {\n",
    "    \"fixed_size\": RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=100\n",
    "    ),\n",
    "    \"sentence\": RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \";\", \":\"],\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=0\n",
    "    ),\n",
    "    \"paragraph\": RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\"],\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "VECTOR_DB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for dir_path in [BASE_DIR, IMPORT_DIR, PROCESSED_DIR, INPUT_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "def list_pdf_files(directory):\n",
    "    return [f for f in directory.glob(\"*.pdf\")]\n",
    "  \n",
    "pdf_files = list_pdf_files(IMPORT_DIR)\n",
    "\n",
    "print(f\"PDF-Dateien: {len(pdf_files)}\")\n",
    "for pdf in pdf_files[:5]:  # Zeige die ersten 5 Dateien\n",
    "    print(f\" - {pdf.name}\")\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text_content = []\n",
    "    \n",
    "    for page_num in range(len(document)):\n",
    "        page = document[page_num]\n",
    "        text = page.get_text()\n",
    "        \n",
    "        # Grundlegende Textbereinigung\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Mehrfach-Leerzeichen entfernen\n",
    "        text = text.strip()\n",
    "        \n",
    "        if text:\n",
    "            text_content.append(f\"--- Seite {page_num + 1} ---\\n{text}\")\n",
    "    \n",
    "    document.close()\n",
    "    \n",
    "    # Gesamten Text zusammenführen\n",
    "    processed_text = \"\\n\\n\".join(text_content)\n",
    "    \n",
    "    # Weitere Bereinigungen für bessere LLM-Verarbeitung\n",
    "    processed_text = re.sub(r'([.!?])\\s*(\\w)', r'\\1\\n\\2', processed_text)  # Satzenden mit Zeilenumbrüchen\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# 4. Dateien verarbeiten und verschieben\n",
    "processed_files = []\n",
    "\n",
    "for pdf_path in pdf_files:\n",
    "    print(f\"Verarbeite: {pdf_path.name}\")\n",
    "    \n",
    "    # Text extrahieren und aufbereiten\n",
    "    processed_text = process_pdf(pdf_path)\n",
    "    \n",
    "    # Ausgabedatei im embeddings-ready Verzeichnis erstellen\n",
    "    output_file = INPUT_DIR / f\"{pdf_path.stem}.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(processed_text)\n",
    "    \n",
    "    # Originaldatei in processed-Verzeichnis verschieben\n",
    "    target_path = PROCESSED_DIR / pdf_path.name\n",
    "    shutil.move(pdf_path, target_path)\n",
    "    \n",
    "    processed_files.append({\n",
    "        \"original_file\": pdf_path.name,\n",
    "        \"processed_file\": output_file.name,\n",
    "        \"size_kb\": round(output_file.stat().st_size / 1024, 2)\n",
    "    })\n",
    "    \n",
    "def load_vector_db(db_path, embedding_model):\n",
    "    \"\"\"Lädt eine existierende Vektordatenbank\"\"\"\n",
    "    print(f\"Lade Vector DB aus {db_path}\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "    db = Chroma(\n",
    "        persist_directory=str(db_path),\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    return db\n",
    "\n",
    "def query_ollama(prompt, model=OLLAMA_MODEL):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/chat\",\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    return response.json()\n",
    "  \n",
    "def search_and_query_llm(db_path, query, embedding_model):\n",
    "    # DB-Suche durchführen\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nSuche in {db_path} nach: '{query}'\")\n",
    "    \n",
    "    db = load_vector_db(db_path, embedding_model)\n",
    "    docs = db.similarity_search(query, k=3)\n",
    "    suchzeit = time.time() - start_time\n",
    "    \n",
    "    # LLM-Antwort generieren\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    prompt = f\"\"\"Basierend auf dem folgenden Kontext, beantworte die Frage.\n",
    "\n",
    "Kontext:\n",
    "{context}\n",
    "\n",
    "Frage: {query}\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = query_ollama(prompt)\n",
    "    antwortzeit = time.time() - start_time\n",
    "    content = response['message']['content']\n",
    "    # Ausgabe formatieren\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"FRAGE:\")\n",
    "    print(\"-\"*60)\n",
    "    print(query)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"ANTWORT:\")\n",
    "    print(\"-\"*60)\n",
    "    print(content)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"METADATEN:\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"Suchzeit:       {suchzeit:.4f} Sekunden\")\n",
    "    print(f\"LLM-Antwortzeit: {antwortzeit:.2f} Sekunden\")\n",
    "    print(f\"Gesamtzeit:     {suchzeit + antwortzeit:.2f} Sekunden\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"suchzeit\": suchzeit,\n",
    "        \"antwortzeit\": antwortzeit,\n",
    "        \"gesamtzeit\": suchzeit + antwortzeit,\n",
    "        \"antwort\": content\n",
    "    }\n",
    "\n",
    "# Dokumente laden\n",
    "def load_documents(directory: Path):\n",
    "    documents = []\n",
    "    for file_path in directory.glob(\"*.txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        doc = Document(page_content=text, metadata={\"source\": file_path.name})\n",
    "        documents.append(doc)\n",
    "    print(f\"Geladen: {len(documents)} Dokumente\")\n",
    "    return documents\n",
    "\n",
    "# Vector DB für ein Embedding-Modell erstellen\n",
    "def create_vector_db(chunks, model_path, db_path):\n",
    "    start_time = time.time()\n",
    "    memory_before = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    print(f\"\\nErstelle Vector DB mit {model_path}\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_path)\n",
    "    \n",
    "    db = Chroma.from_documents(\n",
    "        chunks,\n",
    "        embeddings,\n",
    "        persist_directory=str(db_path)\n",
    "    )\n",
    "    \n",
    "    memory_after = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "    erstellungszeit = time.time() - start_time\n",
    "    \n",
    "    print(f\"Vector DB in {db_path} gespeichert\")\n",
    "    print(f\"Erstellungsdauer: {erstellungszeit:.2f} Sekunden\")\n",
    "    print(f\"Speicherverbrauch: {memory_after - memory_before:.2f} MB\")\n",
    "    \n",
    "    return db, db_path\n",
    "\n",
    "# Hauptfunktion\n",
    "# Dokumente laden und vorbereiten\n",
    "documents = load_documents(INPUT_DIR)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung der Content Embeddings \n",
    "\n",
    "### Leitfragen zur Bewertung\n",
    "\n",
    "### Ausgewählte Embeddings\n",
    "\n",
    "### Ergebnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geladen: 10 Dokumente\n",
      "Dokumente in 222 Chunks aufgeteilt\n",
      "\n",
      "Erstelle Vector DB mit sentence-transformers/all-MiniLM-L6-v2\n",
      "Vector DB in knowledge-base\\vector-stores\\word_level_db gespeichert\n",
      "Erstellungsdauer: 6.01 Sekunden\n",
      "Speicherverbrauch: 412.75 MB\n",
      "\n",
      "Erstelle Vector DB mit sentence-transformers/all-mpnet-base-v2\n",
      "Vector DB in knowledge-base\\vector-stores\\sentence_level_db gespeichert\n",
      "Erstellungsdauer: 67.81 Sekunden\n",
      "Speicherverbrauch: 712.55 MB\n",
      "\n",
      "Erstelle Vector DB mit intfloat/multilingual-e5-large\n",
      "Vector DB in knowledge-base\\vector-stores\\document_level_db gespeichert\n",
      "Erstellungsdauer: 191.03 Sekunden\n",
      "Speicherverbrauch: -617.21 MB\n",
      "\n",
      "=== Vergleichstest aller Vector DBs ===\n",
      "\n",
      "Suche in word_level_db nach: 'Was sind die Hauptvorteile von erneuerbaren Energien?'\n",
      "Suchzeit: 0.0823 Sekunden\n",
      "LLM-Antwortzeit: 2.08 Sekunden\n",
      "Gesamtzeit: 2.16 Sekunden\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'message'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 127\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Vergleichstest aller Vector DBs ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 127\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord-Level\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msearch_and_query_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_db\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTEST_FRAGE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword_level_db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentence-Level\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m search_and_query_llm(sentence_db, TEST_FRAGE, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_level_db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    129\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument-Level\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m search_and_query_llm(document_db, TEST_FRAGE, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_level_db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 90\u001b[0m, in \u001b[0;36msearch_and_query_llm\u001b[1;34m(db, query, db_name)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM-Antwortzeit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mantwortzeit\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Sekunden\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGesamtzeit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuchzeit\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mantwortzeit\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Sekunden\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAntwort: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuchzeit\u001b[39m\u001b[38;5;124m\"\u001b[39m: suchzeit,\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mantwortzeit\u001b[39m\u001b[38;5;124m\"\u001b[39m: antwortzeit,\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgesamtzeit\u001b[39m\u001b[38;5;124m\"\u001b[39m: suchzeit \u001b[38;5;241m+\u001b[39m antwortzeit,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mantwort\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     97\u001b[0m }\n",
      "\u001b[1;31mKeyError\u001b[0m: 'message'"
     ]
    }
   ],
   "source": [
    "# Dokumente in Chunks aufteilen\n",
    "def prepare_chunks(documents):\n",
    "    max_chunk_size = min(CONTEXT_WINDOW // 2, TOKEN_LIMIT)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_chunk_size,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Dokumente in {len(chunks)} Chunks aufgeteilt\")\n",
    "    return chunks\n",
    "\n",
    "chunks = prepare_chunks(documents)\n",
    "\n",
    "for config in EMBEDDING_CONFIGS:\n",
    "    create_vector_db(chunks, config[\"model\"], config[\"db_path\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluierung Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Vergleichstest aller Vector DBs ===\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\word_level_db nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\word_level_db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\campus\\mle\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Martin\\AppData\\Local\\Temp\\ipykernel_39520\\840182271.py:96: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suchzeit: 24.7918 Sekunden\n",
      "LLM-Antwortzeit: 7.02 Sekunden\n",
      "Gesamtzeit: 31.82 Sekunden\n",
      "\n",
      "Antwort: The provided context does not explicitly mention a specific \"callback\" mechanism being used in the models or their implementations. Callback functions are typically associated with libraries like TensorFlow/Keras when working on machine learning projects and would be defined by the user to perform certain actions (like saving model checkpoints, adjusting hyperparameters during training based on performance metrics) at various stages of the neural network's lifecycle.\n",
      "\n",
      "In Keras/TensorFlow specifically:\n",
      "- `ModelCheckpoint` is used for periodically saving models.\n",
      "- The custom callback functions can also include logic like reducing learning rate when a plateau in loss occurs (`ReduceLROnPlateau`) or stopping training after so many epochs if there's no improvement (Custom Callback).\n",
      "\n",
      "However, since the provided context does not directly discuss callbacks but rather focuses on explaining different neural network architectures and their implementations for anomaly detection purposes using autoencoders among others. If you're interested specifically about how to implement a callback function in Keras/TensorFlow or any other machine learning library/framework that uses sequential models like RNNs, LSTMs, GRUs etc., I can certainly guide you through creating one based on your requirements!\n",
      "\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\sentence_level_db nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\sentence_level_db\n",
      "Suchzeit: 2.1457 Sekunden\n",
      "LLM-Antwortzeit: 6.47 Sekunden\n",
      "Gesamtzeit: 8.62 Sekunden\n",
      "\n",
      "Antwort: The provided context does not explicitly mention any specific \"callback functions\" in relation to their use within the described methodologies. Callback functions are typically used with libraries like Keras or TensorFlow in Python when building neural networks for tasks such as model checkpointing, early stopping, logging metrics at certain intervals of batch processing during training.\n",
      "\n",
      "For example:\n",
      "\n",
      "- `ModelCheckpoint`: Saves the current best weights based on performance to a specified file path.\n",
      "- `EarlyStopping`: Monitors a predetermined quantity (e.g., validation loss) and stops training if there is no improvement for given number (`patience`) epochs.\n",
      "- `TensorBoard`: Provides visualization tools that track metrics or visualizations of tensors in time.\n",
      "\n",
      "Since these specific callbacks are not mentioned within the provided context, I cannot determine which callback function was called during their respective trainings. The general idea behind such functions would be to monitor and influence various aspects like model performance on a validation set (to prevent overfitting), saving best-performing models based upon some metric improvement criteria or providing insights into how well training is progressing.\n",
      "\n",
      "If you are interested in learning about the specific callback function used for anomaly detection with autoencoders, as mentioned at the end of Page 7 context section (\"Subsequently, DLAE models are trained on these reconstruction errors\"), it would depend upon which deep-learning framework was employed (such as TensorFlow or PyTorch) and how they were implemented in your project. You could potentially add a callback to save checkpoints if you're interested but please provide more specific details related directly with the context given for an accurate answer regarding that particular use case.\n",
      "\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\document_level_db nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\document_level_db\n",
      "Suchzeit: 4.4895 Sekunden\n",
      "LLM-Antwortzeit: 4.85 Sekunden\n",
      "Gesamtzeit: 9.34 Sekunden\n",
      "\n",
      "Antwort: The provided context does not contain information about a specific machine learning model or its implementation details that would allow us to identify which callback function might be used for monitoring events such as batch end. Callback functions in the context of neural networks and deep learning are typically associated with libraries like TensorFlow/Keras, PyTorch, etc., where they can monitor progress during training.\n",
      "\n",
      "In Keras (a high-level API running on top of TensorFlow), common callbacks include `ModelCheckpoint`, which saves a model at some interval or when it achieves the best performance so far; and `LearningRateScheduler` to change learning rates as you train. In PyTorch, there are equivalent concepts like saving checkpoints with `torch.save()` inside custom checkpointing code blocks.\n",
      "\n",
      "In general terms without specifying an API/library (like TensorFlow/Keras), it's not possible to definitively say which callback function is called during training since this heavily depends on the specific machine learning framework and implementation being used.\n",
      "\n",
      "\n",
      "=== Zusammenfassung ===\n",
      "Embedding-Modell | Suchzeit (s) | Antwortzeit (s) | Gesamtzeit (s)\n",
      "-----------------------------------------------------------------\n",
      "Word-Level      | 24.7918 | 7.02 | 31.82\n",
      "Sentence-Level  | 2.1457 | 6.47 | 8.62\n",
      "Document-Level  | 4.4895 | 4.85 | 9.34\n"
     ]
    }
   ],
   "source": [
    "# Alle drei DBs mit der gleichen Frage testen\n",
    "print(\"\\n=== Vergleichstest aller Vector DBs ===\")\n",
    "\n",
    "results = {}\n",
    "for config in EMBEDDING_CONFIGS:\n",
    "    results[config[\"name\"]] = search_and_query_llm(config[\"db_path\"], TEST_QUERY, config[\"model\"])\n",
    "\n",
    "# Zusammenfassung der Ergebnisse\n",
    "print(\"\\n=== Zusammenfassung ===\")\n",
    "print(\"Embedding-Modell | Suchzeit (s) | Antwortzeit (s) | Gesamtzeit (s)\")\n",
    "print(\"-\" * 65)\n",
    "for model, result in results.items():\n",
    "    print(f\"{model:15} | {result['suchzeit']:.4f} | {result['antwortzeit']:.2f} | {result['gesamtzeit']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung des Chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Chunking-Methode: fixed_size ===\n",
      "Chunks erstellt: 759\n",
      "\n",
      "Erstelle Vector DB mit sentence-transformers/all-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "# Chunking-Methoden testen\n",
    "for method_name, splitter in CHUNKING_METHODS.items():\n",
    "    print(f\"\\n=== Chunking-Methode: {method_name} ===\")\n",
    "    \n",
    "    # Dokumente in Chunks aufteilen\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    print(f\"Chunks erstellt: {len(chunks)}\")\n",
    "    \n",
    "    # Vector-DB erstellen mit bestehender Methode\n",
    "    db, _ = create_vector_db(chunks, EMBEDDING_MODEL, f\"chunking_{method_name}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluierung Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suche in knowledge-base\\vector-stores\\chunking_fixed_size nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\chunking_fixed_size\n",
      "\n",
      "============================================================\n",
      "FRAGE:\n",
      "------------------------------------------------------------\n",
      "Which callback function is called during training?\n",
      "============================================================\n",
      "ANTWORT:\n",
      "------------------------------------------------------------\n",
      "During the process of neural network or machine learning model training, a common practice involves monitoring and reacting to various events that occur. One such event where you might want an action triggered at specific moments (like after each batch processing) happens through what are known as \"callbacks.\" Callbacks serve multiple purposes like saving checkpoints periodically for later restoration in case of failures during the long-running process or adjusting learning rates dynamically based on certain conditions.\n",
      "\n",
      "Specifically, there is a special kind of callback function called `on_train_batch_end`. This particular callback gets executed at different stages throughout each iteration (or epoch) and after every batch has been processed. Its primary purpose within an iterative training loop context such as with Keras or TensorFlow's fit methods would typically involve logging metrics like loss, accuracy for the current batches so far; saving models periodically to avoid losing progress in case of interruption.\n",
      "\n",
      "Another frequently encountered callback is `on_epoch_end`. It's used somewhat similarly but acts at a higher level: after an epoch (a full pass through all training samples) finishes. It can be useful when you want something like adjusting learning rate, plotting metrics across epochs for visual inspection or even implementing early stopping mechanisms if the performance starts to degrade.\n",
      "\n",
      "It's crucial in model development and deployment environments that callbacks are used correctly as they allow fine-grained control over these processes without interrupting core training loops directly which might otherwise cause unwanted side effects.\n",
      "============================================================\n",
      "METADATEN:\n",
      "------------------------------------------------------------\n",
      "Suchzeit:       1.8212 Sekunden\n",
      "LLM-Antwortzeit: 4.84 Sekunden\n",
      "Gesamtzeit:     6.66 Sekunden\n",
      "============================================================\n",
      "\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\chunking_sentence nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\chunking_sentence\n",
      "\n",
      "============================================================\n",
      "FRAGE:\n",
      "------------------------------------------------------------\n",
      "Which callback function is called during training?\n",
      "============================================================\n",
      "ANTWORT:\n",
      "------------------------------------------------------------\n",
      "The specific context provided does not specify which particular programming language or machine learning framework being referred to. However, in many popular deep learning frameworks such as TensorFlow and PyTorch:\n",
      "\n",
      "- In **PyTorch**, the primary callback functions that are commonly used for tracking progress (such as loss) can be set up using `torch.utils.tensorboard.SummaryWriter` alongside a custom function within your training loop.\n",
      "\n",
      "  ```python\n",
      "  def log_training_loss(epoch, batch_i):\n",
      "      # Code to calculate and record epoch/loss information goes here.\n",
      "      \n",
      "   ```\n",
      "\n",
      "- In **TensorFlow/Keras**, you would typically use the built-in callbacks provided by Keras:\n",
      "\n",
      "  ```python\n",
      "  from tensorflow.keras.callbacks import TensorBoard\n",
      "\n",
      "  tensorboard_callback = TensorBoard(log_dir=\"logs\", update_freq='batch')\n",
      "  \n",
      "  # During model.fit(), include:\n",
      "  model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), callbacks=[tensorboard_callback])\n",
      "  ```\n",
      "\n",
      "Without more specific details regarding the context (such as whether it's PyTorch or TensorFlow/Keras you're referring to) and what exactly \"training\" entails in terms of your task's requirements. Please provide further information so I can give you a precise answer related specifically to that situation.\n",
      "============================================================\n",
      "METADATEN:\n",
      "------------------------------------------------------------\n",
      "Suchzeit:       1.7828 Sekunden\n",
      "LLM-Antwortzeit: 4.70 Sekunden\n",
      "Gesamtzeit:     6.48 Sekunden\n",
      "============================================================\n",
      "\n",
      "\n",
      "Suche in knowledge-base\\vector-stores\\chunking_paragraph nach: 'Which callback function is called during training?'\n",
      "Lade Vector DB aus knowledge-base\\vector-stores\\chunking_paragraph\n",
      "\n",
      "============================================================\n",
      "FRAGE:\n",
      "------------------------------------------------------------\n",
      "Which callback function is called during training?\n",
      "============================================================\n",
      "ANTWORT:\n",
      "------------------------------------------------------------\n",
      "In den meisten Deep-Learning-Bibliotheken wie TensorFlow oder PyTorch wird während des Trainings ein Callback-Funktion (oder eine Serie von Callbacks) verwendet. Ein häufig verwendeter Callback ist das `ModelCheckpoint`-Callback, mit dem man die Modellversionen speichern kann, bei denen es sich um signifikante Verbesserungen handelt.\n",
      "\n",
      "Ein weiteres Beispiel wäre der `ReduceLROnPlateau` Callback in PyTorch oder TensorFlow-Keras, der den Lernrate reduziert, wenn bestimmte Kriterien erfüllt sind (zum Beispiel kein Fortschritt des Verlusts).\n",
      "\n",
      "Hier ein einfaches Python-Beispiel unter Verwendung von Keras:\n",
      "\n",
      "```python\n",
      "from keras.callbacks import ModelCheckpoint\n",
      "\n",
      "# Definiere deinen Modell-Supporter und deine Callbacks-Liste etc.\n",
      "checkpoint = ModelCheckpoint(filepath='path_to_save_model.h5', \n",
      "                              save_best_only=True, monitor='val_loss', mode='min')\n",
      "\n",
      "callbacks_list = [checkpoint]\n",
      "\n",
      "model.fit(X_train, y_train,\n",
      "          validation_data=(X_val, y_val),\n",
      "          epochs=10,\n",
      "          callbacks=callbacks_list)\n",
      "```\n",
      "\n",
      "In diesem Beispiel wird der Callback `ModelCheckpoint` jede Zeit aufgerufen und speichert den besten Modellen basierend auf dem Verlust in einem gültigen Trainingsset. Dies gewährleistet, dass du nach jedem Training-Schritt die beste Version des Modells hast.\n",
      "\n",
      "Es gibt jedoch viele andere Callbacks für verschiedene Zwecke wie EarlyStopping (um zu stoppen), LearningRateScheduler (zum Einstellen der Lernrate während des Trainings) und so weiter. Der genutzte Callback hängt von den spezifischen Anforderungen deines Projekts ab.\n",
      "============================================================\n",
      "METADATEN:\n",
      "------------------------------------------------------------\n",
      "Suchzeit:       1.7118 Sekunden\n",
      "LLM-Antwortzeit: 5.38 Sekunden\n",
      "Gesamtzeit:     7.09 Sekunden\n",
      "============================================================\n",
      "\n",
      "\n",
      "=== Zusammenfassung der Chunking-Methoden ===\n",
      "Methode        | Suchzeit (s) | Antwortzeit (s) | Gesamtzeit (s)\n",
      "-----------------------------------------------------------------\n",
      "fixed_size      | 1.8212 | 4.84 | 6.66\n",
      "sentence        | 1.7828 | 4.70 | 6.48\n",
      "paragraph       | 1.7118 | 5.38 | 7.09\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = {}\n",
    "for method_name, splitter in CHUNKING_METHODS.items():\n",
    "    # Mit LLM testen\n",
    "    results[method_name] = search_and_query_llm(VECTOR_DB_DIR / f\"chunking_{method_name}\", TEST_QUERY, EMBEDDING_MODEL)\n",
    "        \n",
    "# Zusammenfassung\n",
    "print(\"\\n=== Zusammenfassung der Chunking-Methoden ===\")\n",
    "print(\"Methode        | Suchzeit (s) | Antwortzeit (s) | Gesamtzeit (s)\")\n",
    "print(\"-\" * 65)\n",
    "for method, result in results.items():\n",
    "    print(f\"{method:15} | {result['suchzeit']:.4f} | {result['antwortzeit']:.2f} | {result['gesamtzeit']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
